# Sub-Component 3.3: LLM Orchestrator

## Purpose
Manage all LLM API interactions including execution, retries, rate limiting, provider fallback, and resource tracking.

---

## Requirements Gathering

### Question 1: Analysis Time Limits

**What is the maximum acceptable analysis time per chapter (assume 50 pages)?**

Options:
- **A. Fast:** 2-3 minutes (sacrifice depth for speed)
- **B. Balanced:** 5-7 minutes (standard depth)
- **C. Thorough:** 10-15 minutes (maximum detail)
- **D. User choice:** Default balanced, allow user to select mode

Considerations:
- Longer = better quality but user may lose patience
- Shorter = faster but may miss details
- Provider differences (Gemini faster than OpenAI for images)

### Question 2: Batch Processing Strategy

**Should we support parallel batch processing or strictly sequential?**

Options:
- **A. Sequential only:** One batch at a time (simpler, lower API load)
- **B. Limited parallel:** 2-3 batches concurrently (balanced)
- **C. Full parallel:** All batches at once (fastest, highest API load)
- **D. Adaptive:** Sequential for context-heavy, parallel for independent

Considerations:
- Parallel = faster but harder to maintain context
- Sequential = better continuity but slower
- Rate limits may restrict parallel processing

### Question 3: Provider Fallback Strategy

**When should we switch to fallback provider vs retry?**

Options:
- **A. Retry always:** 3 retries on same provider before fallback
- **B. Smart fallback:** Retry transient errors, fallback on auth/context errors
- **C. Immediate fallback:** Switch provider on first error
- **D. User preference:** Primary only, or auto-fallback enabled

Considerations:
- Some errors (rate limit) benefit from retry
- Some errors (context length) won't resolve with retry
- Provider costs may differ

### Question 4: Local LLM Support

**Should we support local LLM models as an option?**

Options:
- **A. No:** Cloud providers only
- **B. Experimental:** Support but mark as beta/unreliable
- **C. Full support:** First-class local LLM integration
- **D. Fallback only:** Use local when cloud unavailable

Considerations:
- Local models (Ollama, LM Studio) offer privacy
- Quality varies significantly
- Requires significant local resources
- Multi-modal local models still limited

### Question 5: Streaming Responses

**Should we support streaming responses from LLM?**

Options:
- **A. No:** Wait for complete response
- **B. Progress streaming:** Stream progress updates only
- **C. Full streaming:** Stream tokens as they arrive
- **D. Provider-dependent:** Stream if provider supports

Considerations:
- Streaming = better UX but harder with JSON parsing
- Progress updates sufficient for user feedback
- Most LLM APIs support streaming

### Question 6: Token Usage & Cost Tracking

**How should we track and display token usage and costs?**

Options:
- **A. Hidden:** Internal only, for debugging
- **B. Summary:** Show total cost after analysis
- **C. Real-time:** Show cost accumulating during analysis
- **D. Detailed:** Per-batch cost breakdown + total

Considerations:
- Transparency helps users understand costs
- Real-time may distract from analysis
- Detailed breakdown useful for optimization

### Question 7: Cancellation Behavior

**What happens when user cancels analysis mid-process?**

Options:
- **A. Discard all:** Delete any partial results
- **B. Save partial:** Keep completed batches, discard current
- **C. Resume capable:** Save state, allow resume later
- **D. User choice:** Ask on cancel what to do

Considerations:
- Partial results = wasted API spend if discarded
- Resume adds complexity
- User may want to restart with different settings

---

## Technical Considerations

### Rate Limit Handling

| Provider | RPM Limit | TPM Limit | Strategy |
|----------|-----------|-----------|----------|
| Gemini 1.5 Pro | 60 | 1M | Monitor headers, backoff |
| GPT-4o | 500 | 30K | Token bucket algorithm |

### Retry Configuration

```typescript
interface RetryConfig {
  maxRetries: number;           // default: 3
  baseDelayMs: number;          // default: 1000
  maxDelayMs: number;           // default: 60000
  backoffMultiplier: number;    // default: 2
  
  // Which errors trigger retry
  retryableErrors: ErrorType[];
  
  // Which errors trigger fallback
  fallbackErrors: ErrorType[];
}
```

---

## Decisions Recorded

| Question | Decision | Rationale |
|----------|----------|-----------|
| 1. Analysis Time | **C. Thorough (10-15 min)** | Maximum detail for downstream quality |
| 2. Batch Processing | **A. Sequential only** | Maximum context continuity between batches |
| 3. Fallback Strategy | **A. Retry always (3× + exponential backoff)** | Give primary provider fair chance before fallback |
| 4. Local LLM | **A. No** | Cloud providers only for reliability |
| 5. Streaming | **A. No** | Wait for complete response for reliable JSON |
| 6. Cost Tracking | **D. Detailed** | Per-batch breakdown + total for transparency |
| 7. Cancellation | **D. User choice** | Ask on cancel: save partial / discard / resume later |

### Retry Configuration with Exponential Backoff

```typescript
const RETRY_CONFIG: RetryConfig = {
  maxRetries: 3,
  baseDelayMs: 1000,
  maxDelayMs: 30000,
  backoffMultiplier: 2,
  
  // Delays: 1s, 2s, 4s (before next attempt)
  calculateDelay: (attempt: number) => {
    const delay = Math.min(
      baseDelayMs * Math.pow(backoffMultiplier, attempt - 1),
      maxDelayMs
    );
    // Add jitter to avoid thundering herd
    return delay + Math.random() * 1000;
  },
  
  retryableErrors: [
    'rate_limit',
    'timeout',
    'temporary_unavailable',
    'network_error'
  ],
  
  fallbackErrors: [
    'invalid_auth',
    'context_length_exceeded',
    'content_filtered',
    'max_retries_exceeded'
  ]
};
```

### Analysis Modes (Future: User-Selectable)

```typescript
type AnalysisMode = 'thorough'; // Current default

// Future expansion:
// type AnalysisMode = 'quick' | 'balanced' | 'thorough';
// 
// Quick: 50 pages, fewer passes, ~3 minutes
// Balanced: 50 pages, standard passes, ~7 minutes  
// Thorough: 50 pages, multiple passes, ~12 minutes (current)
```

### Sequential Batch Processing Flow

```
Batch 1 → Wait → Parse → Store
   ↓
Batch 2 → Wait → Parse → Merge
   ↓
Batch 3 → Wait → Parse → Merge
   ↓
Final Merge → Quality Check → Complete

Context from each batch feeds into next batch prompt
```

### Cost Tracking Display

```
Analyzing Manga Title...

Progress: Batch 3 of 5 (60%)

Cost Breakdown:
  Batch 1: $0.12 (1,234 tokens)
  Batch 2: $0.15 (1,567 tokens)
  Batch 3: In progress...
  
Total so far: $0.27
Estimated total: $0.45

[Cancel Analysis]
```

### Cancel Dialog

```
Cancel Analysis?

You have completed 3 of 5 batches.
Current cost: $0.27

[Save Partial Results]  → Keep completed analysis
[Discard All]           → Delete everything
[Resume Later]          → Save state, continue later
[Continue Analysis]     → Don't cancel
```

---

## Status: ✅ Interrogation Complete

**Next: Sub-Component 3.4 — Response Parser**
